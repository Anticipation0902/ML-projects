{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7168030b",
   "metadata": {},
   "source": [
    "#  Diving into the basic functions and utilities in pyspark\n",
    "\n",
    "Assuming you downloaded the pyspark package, to start it you must first create a 'SparkSession'. Just copy the below code to get started. The appName 'test' that I used can be changed to a suitable name for the ETL project.\n",
    "\n",
    "Note: Your SparkSession is now stored in the variable ```spark```. This variable will be what you call to read data into pyspark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fea3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SparkSession to begin data processing\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b731a",
   "metadata": {},
   "source": [
    "Simply calling ```spark``` below displays the version & specifications of the SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd33a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HKPC0P4A6T.cn.asia.ad.pwcinternal.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2359ca0fd00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ac37e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Load and inspect dataset from csv, using pyspark dataframe\n",
    "In my directory there is a csv file containing fake employees data, including their ID, Name, Age, Gender, Salary and Spending (in thousands), and their Department specialization (ML- Machine Learning, Fin- Finance, CS- Computer Science). We will load this data into a dataframe 'df_ps' below. \n",
    "\n",
    "Setting ```header = True``` allows the dataframe ```df_ps``` to recognize the 'Name', 'Age', etc. as the column name of a column instead of entries like the rows below it. Setting ```inferSchema = True``` allows pyspark to recognize numerical entries, e.g. 'Age' as integers (in this case) instead of string.\n",
    "\n",
    "The show command displays the full table when the table is small enough. Otherwise it will only display the first 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2350b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|\n",
      "|  211|  James|null|     M|  null|      70| Fin|\n",
      "|  311|   null|  50|  null|   400|     102| Fin|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|\n",
      "|  199|   null|null|  null|   105|    null|  ML|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps = spark.read.csv('employees.csv', header = True, inferSchema = True)\n",
    "df_ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0817ec",
   "metadata": {},
   "source": [
    "**Remark:** pyspark supports loading data from txt (and any other format) and the method is no different except you specify tab as delimeter via ```option(\"sep\",\"\\t\")```. The functionality of pyspark tools is the same regardless of initial format your data is stored in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4120656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+-----+\n",
      "|CustomerName|ProductID| Product|Price|\n",
      "+------------+---------+--------+-----+\n",
      "|        John|     W311|  Wallet|  200|\n",
      "|      Jordan|     B203|    Belt|  500|\n",
      "|      Rachel|     B133|     Bag| 1000|\n",
      "|      Gordon|     B139|     Bag| 1200|\n",
      "|         Sam|     B155|     Bag| 2300|\n",
      "|       Emily|     S152|   Shoes|  800|\n",
      "|       Alice|     W320|  Wallet|  250|\n",
      "|         Bob|     W320|  Wallet|  250|\n",
      "|      Daniel|     S300|Suitcase| 1200|\n",
      "|       Laura|     S305|Suitcase| 1790|\n",
      "|       Zosso|     W231|   Watch|  420|\n",
      "|       Evans|     DN18|    null| 2000|\n",
      "|       Keith|     null|  Burger|   50|\n",
      "|        null|     P220| Perfume| 1350|\n",
      "+------------+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer = spark.read.option(\"sep\",\"\\t\").csv('customers.txt', header = True, inferSchema = True)\n",
    "df_customer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4ae17",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Returning to our smaller dataset df_ps, ```type(...)``` checks the type of object in the brackets. Our dataframe ```df_ps``` is a pyspark dataframe, as expected. Similarly, we can look at the schema of the dataframe (the metadata standard), and the names of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351ad393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19d5646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Spending: integer (nullable = true)\n",
      " |-- Dept: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7625fd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empID', 'Name', 'Age', 'Gender', 'Salary', 'Spending', 'Dept']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878a07e",
   "metadata": {},
   "source": [
    "We can also check the datatype of the entries in each column via ```dtypes```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88295af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empID', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Gender', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Spending', 'int'),\n",
       " ('Dept', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55242b10",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Here's two ways to display the first few rows of dataframe (great for huge tables). The first is a better visualization and the second reveals more structure: the dataframe is an array of rows objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def7c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---+------+------+--------+----+\n",
      "|empID|   Name|Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "|   13|Hatsune| 25|     F|   245|      50|  ML|\n",
      "|   23|  Alice| 30|     F|   270|      88|  ML|\n",
      "|  115|    Bob| 23|     M|   120|      46| Fin|\n",
      "|  123|Charlie| 25|     M|   133|      39|  CS|\n",
      "|  151| Gordon| 26|     M|   142|      35|  CS|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7307a945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empID=13, Name='Hatsune', Age=25, Gender='F', Salary=245, Spending=50, Dept='ML'),\n",
       " Row(empID=23, Name='Alice', Age=30, Gender='F', Salary=270, Spending=88, Dept='ML'),\n",
       " Row(empID=115, Name='Bob', Age=23, Gender='M', Salary=120, Spending=46, Dept='Fin'),\n",
       " Row(empID=123, Name='Charlie', Age=25, Gender='M', Salary=133, Spending=39, Dept='CS'),\n",
       " Row(empID=151, Name='Gordon', Age=26, Gender='M', Salary=142, Spending=35, Dept='CS')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458edc0",
   "metadata": {},
   "source": [
    "Side note: You can always visualize better after converting to panda dataframe. In fact, it only takes 2 lines to produce a histogram in panda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a755dbb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Here, suppose we only care about the department of an employee and their salary (because we want to see if majoring in CS is rewarding enough...). We can extract these columns with the ```select``` function. Note: we store this information in ```df_deptSalaryINDV``` instead of ```df_ps```. As expected, ```df_deptSalaryINDV``` is also a pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dca8836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Dept|Salary|\n",
      "+----+------+\n",
      "|  ML|   245|\n",
      "|  ML|   270|\n",
      "| Fin|   120|\n",
      "|  CS|   133|\n",
      "|  CS|   142|\n",
      "| Fin|   151|\n",
      "| Fin|   111|\n",
      "|  CS|   124|\n",
      "|  ML|   151|\n",
      "|  CS|   217|\n",
      "|  ML|   164|\n",
      "| Fin|  null|\n",
      "| Fin|   400|\n",
      "|  ML|  1000|\n",
      "|  CS|  null|\n",
      "|  ML|   105|\n",
      "|  ML|  null|\n",
      "|  CS|   135|\n",
      "| Fin|   100|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_deptSalaryINDV = df_ps.select(['Dept', 'Salary'])\n",
    "df_deptSalaryINDV.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c0db15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_deptSalaryINDV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf06105",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A quick way to summarize our data is with the ```describe()``` function. This outputs a new summary table below. Note: categorical variables such as Name, Gender will not have mean and standard dev, and their min/max will just be ordered alphabetically (compared as string types...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca31da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+------------------+------+------------------+------------------+----+\n",
      "|summary|             empID| Name|               Age|Gender|            Salary|          Spending|Dept|\n",
      "+-------+------------------+-----+------------------+------+------------------+------------------+----+\n",
      "|  count|                19|   17|                14|    17|                16|                17|  19|\n",
      "|   mean|138.68421052631578| null|31.142857142857142|  null|             223.0| 56.23529411764706|null|\n",
      "| stddev| 83.57302371218634| null| 8.094225319107661|  null|221.51719271123554|22.456985026280535|null|\n",
      "|    min|                13|Alice|                23|     F|               100|                30|  CS|\n",
      "|    max|               311|Zosso|                50|     M|              1000|               102|  ML|\n",
      "+-------+------------------+-----+------------------+------+------------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bbd006",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Modifying our dataframe\n",
    "\n",
    "Suppose we want to add a column of salary projection of employees in 5 years, given by x1.2 of their current ones. We showcase this below with the ```withColumn``` function. Note that here we are replacing our dataframe ```df_ps``` by a new one with the added column Proj5year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69cf1893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+------------------+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|         Proj5year|\n",
      "+-----+-------+----+------+------+--------+----+------------------+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|             294.0|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|             324.0|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|             144.0|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|             159.6|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|             170.4|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|             181.2|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|             133.2|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|148.79999999999998|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|             181.2|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|             260.4|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|196.79999999999998|\n",
      "|  211|  James|null|     M|  null|      70| Fin|              null|\n",
      "|  311|   null|  50|  null|   400|     102| Fin|             480.0|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|            1200.0|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|              null|\n",
      "|  199|   null|null|  null|   105|    null|  ML|             126.0|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|              null|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|             162.0|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|             120.0|\n",
      "+-----+-------+----+------+------+--------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps = df_ps.withColumn(\"Proj5year\", df_ps['Salary']*1.2)\n",
    "df_ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e23292",
   "metadata": {},
   "source": [
    "After adding new column we realize that the Proj5year column has decimal values. Suppose we want integers instead, then simply use ```withColumn``` function again, this time replacing the column with itself, but with values casted to integers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db601793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+---------+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|Proj5year|\n",
      "+-----+-------+----+------+------+--------+----+---------+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|      294|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|      324|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|      144|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|      159|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|      170|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|      181|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|      133|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|      148|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|      181|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|      260|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|      196|\n",
      "|  211|  James|null|     M|  null|      70| Fin|     null|\n",
      "|  311|   null|  50|  null|   400|     102| Fin|      480|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|     1200|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|     null|\n",
      "|  199|   null|null|  null|   105|    null|  ML|      126|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|     null|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|      162|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|      120|\n",
      "+-----+-------+----+------+------+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps = df_ps.withColumn(\"Proj5year\", df_ps['Proj5year'].cast('integer'))\n",
    "df_ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd516edb",
   "metadata": {},
   "source": [
    "We can drop columns too, and below with ```drop``` we removed all columns that are non-numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b341fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+--------+---------+\n",
      "|empID| Age|Salary|Spending|Proj5year|\n",
      "+-----+----+------+--------+---------+\n",
      "|   13|  25|   245|      50|      294|\n",
      "|   23|  30|   270|      88|      324|\n",
      "|  115|  23|   120|      46|      144|\n",
      "|  123|  25|   133|      39|      159|\n",
      "|  151|  26|   142|      35|      170|\n",
      "|  152|  27|   151|      40|      181|\n",
      "|  156|  26|   111|      35|      133|\n",
      "|  119|  30|   124|      60|      148|\n",
      "|  157|  31|   151|      62|      181|\n",
      "|  123|  40|   217|      45|      260|\n",
      "|  213|  28|   164|      61|      196|\n",
      "|  211|null|  null|      70|     null|\n",
      "|  311|  50|   400|     102|      480|\n",
      "|   67|  45|  1000|    null|     1200|\n",
      "|  143|null|  null|     100|     null|\n",
      "|  199|null|   105|    null|      126|\n",
      "|  289|null|  null|      56|     null|\n",
      "|   55|null|   135|      30|      162|\n",
      "|   15|  30|   100|      37|      120|\n",
      "+-----+----+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.drop('Name', 'Gender', 'Dept').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b2c0a",
   "metadata": {},
   "source": [
    "Similarly, we can rename a column, e.g. Name, with the ```withColumnRenamed``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "828c6840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+---------+\n",
      "|empID|RENAMED| Age|Gender|Salary|Spending|Dept|Proj5year|\n",
      "+-----+-------+----+------+------+--------+----+---------+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|      294|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|      324|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|      144|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|      159|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|      170|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|      181|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|      133|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|      148|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|      181|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|      260|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|      196|\n",
      "|  211|  James|null|     M|  null|      70| Fin|     null|\n",
      "|  311|   null|  50|  null|   400|     102| Fin|      480|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|     1200|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|     null|\n",
      "|  199|   null|null|  null|   105|    null|  ML|      126|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|     null|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|      162|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|      120|\n",
      "+-----+-------+----+------+------+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.withColumnRenamed('Name', 'RENAMED').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff044e93",
   "metadata": {},
   "source": [
    "### Dealing with null values\n",
    "\n",
    "We return to the original employees dataset with Proj5year not present. This table can be used as reference to compare with the subsequent ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b18a823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|\n",
      "|  211|  James|null|     M|  null|      70| Fin|\n",
      "|  311|   null|  50|  null|   400|     102| Fin|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|\n",
      "|  199|   null|null|  null|   105|    null|  ML|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps = spark.read.csv('employees.csv', header = True, inferSchema = True)\n",
    "df_ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a3b3c",
   "metadata": {},
   "source": [
    "Lets first inspect how many null values are in each column of dataframe. There is 2 ways to do this. First, we can pass the dataframe to panda via df_ps.toPandas() and then use ```isnull().sum()```. This can be slow for large datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d8f4f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empID       0\n",
       "Name        2\n",
       "Age         5\n",
       "Gender      2\n",
       "Salary      3\n",
       "Spending    2\n",
       "Dept        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df_ps.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa3811",
   "metadata": {},
   "source": [
    "Second way is more standard sql. We use ```when``` clause which is analogous to sql ```where```. Here we use the strategy of selecting the counts of each column after ```when``` converts nulls to non-nulls (here to ```1```) and vice versa in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59834cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+------+--------+----+\n",
      "|empID|Name|Age|Gender|Salary|Spending|Dept|\n",
      "+-----+----+---+------+------+--------+----+\n",
      "|    0|   2|  5|     2|     3|       2|   0|\n",
      "+-----+----+---+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, count, when\n",
    "\n",
    "df_ps.select([count(when(isnull(c), 1)).alias(c) for c in df_ps.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b59237",
   "metadata": {},
   "source": [
    "The simplest form of drop is to drop rows with ```any``` instance of null values (by default ```how = 'any'```). If we specify ```how = 'all'```, then we only delete rows with all null values. Note: ```dropna()``` is shorthand for ```na.drop()```, and they both work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56a7993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---+------+------+--------+----+\n",
      "|empID|   Name|Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "|   13|Hatsune| 25|     F|   245|      50|  ML|\n",
      "|   23|  Alice| 30|     F|   270|      88|  ML|\n",
      "|  115|    Bob| 23|     M|   120|      46| Fin|\n",
      "|  123|Charlie| 25|     M|   133|      39|  CS|\n",
      "|  151| Gordon| 26|     M|   142|      35|  CS|\n",
      "|  152| Violet| 27|     F|   151|      40| Fin|\n",
      "|  156| Samuel| 26|     M|   111|      35| Fin|\n",
      "|  119| Rachel| 30|     F|   124|      60|  CS|\n",
      "|  157| Daniel| 31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn| 40|     M|   217|      45|  CS|\n",
      "|  213| Shuang| 28|     F|   164|      61|  ML|\n",
      "|   15|  Laura| 30|     F|   100|      37| Fin|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33496728",
   "metadata": {},
   "source": [
    "The next parameter is ```thresh```. When specified, a row will be deleted whenever (num of non-null values in row) < thresh. As shown below, you should compare with the original dataset [82] to check that this is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d3c5c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|\n",
      "|  211|  James|null|     M|  null|      70| Fin|\n",
      "|  311|   null|  50|  null|   400|     102| Fin|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.dropna(thresh = 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4c87f",
   "metadata": {},
   "source": [
    "You can also remove null values in specific columns by specifying ```subset```. Below we only removed them for Name and Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "628dec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.dropna(subset = ['Name', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981542e",
   "metadata": {},
   "source": [
    "Instead of dropping, we can also fill the null values. We also do multiple instance of fill, via dictionary as shown.\n",
    "\n",
    "What the dictionary below does is replace all null in Name with Steve, and all null  in Gender with M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "915aa777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|\n",
      "|  211|  James|null|     M|  null|      70| Fin|\n",
      "|  311|  Steve|  50|     M|   400|     102| Fin|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|\n",
      "|  199|  Steve|null|     M|   105|    null|  ML|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|\n",
      "|   55|  Keith|null|     M|   135|      30|  CS|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.fillna({'Name': 'Steve', 'Gender': 'M'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f75d1",
   "metadata": {},
   "source": [
    "```Imputer``` can also be used to replace null values, and below we select the columns to impute, declare the strategy as ```median``` (which replaces all null values with the median of non-nulls), and proceed to transform our dataframe according to this strategy. Another commonly used strategy is ```mean```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6275f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---+------+------+--------+----+\n",
      "|empID|   Name|Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "|   13|Hatsune| 25|     F|   245|      50|  ML|\n",
      "|   23|  Alice| 30|     F|   270|      88|  ML|\n",
      "|  115|    Bob| 23|     M|   120|      46| Fin|\n",
      "|  123|Charlie| 25|     M|   133|      39|  CS|\n",
      "|  151| Gordon| 26|     M|   142|      35|  CS|\n",
      "|  152| Violet| 27|     F|   151|      40| Fin|\n",
      "|  156| Samuel| 26|     M|   111|      35| Fin|\n",
      "|  119| Rachel| 30|     F|   124|      60|  CS|\n",
      "|  157| Daniel| 31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn| 40|     M|   217|      45|  CS|\n",
      "|  213| Shuang| 28|     F|   164|      61|  ML|\n",
      "|  211|  James| 28|     M|   142|      70| Fin|\n",
      "|  311|   null| 50|  null|   400|     102| Fin|\n",
      "|   67| Andrew| 45|     M|  1000|      50|  ML|\n",
      "|  143|  Stacy| 28|     F|   142|     100|  CS|\n",
      "|  199|   null| 28|  null|   105|      50|  ML|\n",
      "|  289|  Zosso| 28|     M|   142|      56|  ML|\n",
      "|   55|  Keith| 28|     M|   135|      30|  CS|\n",
      "|   15|  Laura| 30|     F|   100|      37| Fin|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=['Age', 'Salary', 'Spending'], outputCols=['Age', 'Salary', 'Spending'])\n",
    "imputer.setStrategy('median')\n",
    "\n",
    "model = imputer.fit(df_ps)\n",
    "model.transform(df_ps).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b07bfe",
   "metadata": {},
   "source": [
    "\n",
    "### Filtering dataframe by conditions/clause\n",
    "\n",
    "The ```filter()``` function only takes in one parameter, which can be any set of conditions which it will filter by. The example below should prove enough to explain the syntax of the function, which you can generalize.\n",
    "Note: The \"not\" logical operator is ~, which I did not use\n",
    "\n",
    "Suppose we only want to know the name of all male, not older than 30, with salary less than 150K or more than 800K. We also want the department they belong to, but we don't care *exactly* how much they earn. We can ```filter``` the dataframe then ```select``` the relevant columns, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9516f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   Name|Dept|\n",
      "+-------+----+\n",
      "|    Bob| Fin|\n",
      "|Charlie|  CS|\n",
      "| Gordon|  CS|\n",
      "| Samuel| Fin|\n",
      "| Andrew|  ML|\n",
      "|  Keith|  CS|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.filter((df_ps['Gender'] == 'M') & ((df_ps['Salary'] < 150) | (df_ps['Salary'] > 800))).select(['Name', 'Dept']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7522d0",
   "metadata": {},
   "source": [
    "\n",
    "### Various methods of replacements\n",
    "\n",
    "The easiest method we will look at is a direct replace via ```replace()``` function. For example, you want to replace all instances of CS with SWE and Fin with Bus. You can do what is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e501e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   120|      46| Bus|\n",
      "|  123|Charlie|  25|     M|   133|      39| SWE|\n",
      "|  151| Gordon|  26|     M|   142|      35| SWE|\n",
      "|  152| Violet|  27|     F|   151|      40| Bus|\n",
      "|  156| Samuel|  26|     M|   111|      35| Bus|\n",
      "|  119| Rachel|  30|     F|   124|      60| SWE|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   217|      45| SWE|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|\n",
      "|  211|  James|null|     M|  null|      70| Bus|\n",
      "|  311|   null|  50|  null|   400|     102| Bus|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|\n",
      "|  143|  Stacy|null|     F|  null|     100| SWE|\n",
      "|  199|   null|null|  null|   105|    null|  ML|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|\n",
      "|   55|  Keith|null|     M|   135|      30| SWE|\n",
      "|   15|  Laura|  30|     F|   100|      37| Bus|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.replace(['CS', 'Fin'], ['SWE', 'Bus']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b799f2",
   "metadata": {},
   "source": [
    "```replace``` also has a third parameter called subset to narrow down to the columns which you want to consider replacements. It has not only practical use, but it is good practice to do this since it takes a very long time to scan a large file; scanning a column can save significant time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbe45db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   120|      46| Bus|\n",
      "|  123|Charlie|  25|     M|   133|      39| SWE|\n",
      "|  151| Gordon|  26|     M|   142|      35| SWE|\n",
      "|  152| Violet|  27|     F|   151|      40| Bus|\n",
      "|  156| Samuel|  26|     M|   111|      35| Bus|\n",
      "|  119| Rachel|  30|     F|   124|      60| SWE|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   217|      45| SWE|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|\n",
      "|  211|  James|null|     M|  null|      70| Bus|\n",
      "|  311|   null|  50|  null|   400|     102| Bus|\n",
      "|   67| Andrew|  45|     M|  1000|    null|  ML|\n",
      "|  143|  Stacy|null|     F|  null|     100| SWE|\n",
      "|  199|   null|null|  null|   105|    null|  ML|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|\n",
      "|   55|  Keith|null|     M|   135|      30| SWE|\n",
      "|   15|  Laura|  30|     F|   100|      37| Bus|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#same output as above replace(), since CS, Fin is uniquely present in Dept\n",
    "df_ps.replace(['CS', 'Fin'], ['SWE', 'Bus'], ['Dept']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4be63d",
   "metadata": {},
   "source": [
    "#### Replacement given more types of conditions/clauses\n",
    "\n",
    "If we want to do something more complex, like replace salary by 0 if it's less than 200K and the dept is ML, and double salary otherwise, then we can do this via a combination of ```withColumn``` for replacement of a column and ```when``` function to specify 1st the clause, and 2nd what to do if clause is satisfied, and ```otherwise``` to specify what to do if clause not satisfied.\n",
    "\n",
    "Note: If you want to keep the values which fails the clause non-null, you must specify what happens via ```.otherwise```!\n",
    "\n",
    "The syntax looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "239b3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "|   13|Hatsune|  25|     F|   490|      50|  ML|\n",
      "|   23|  Alice|  30|     F|   540|      88|  ML|\n",
      "|  115|    Bob|  23|     M|   240|      46| Fin|\n",
      "|  123|Charlie|  25|     M|   266|      39|  CS|\n",
      "|  151| Gordon|  26|     M|   284|      35|  CS|\n",
      "|  152| Violet|  27|     F|   302|      40| Fin|\n",
      "|  156| Samuel|  26|     M|   222|      35| Fin|\n",
      "|  119| Rachel|  30|     F|   248|      60|  CS|\n",
      "|  157| Daniel|  31|     M|     0|      62|  ML|\n",
      "|  123|  Glenn|  40|     M|   434|      45|  CS|\n",
      "|  213| Shuang|  28|     F|     0|      61|  ML|\n",
      "|  211|  James|null|     M|  null|      70| Fin|\n",
      "|  311|   null|  50|  null|   800|     102| Fin|\n",
      "|   67| Andrew|  45|     M|  2000|    null|  ML|\n",
      "|  143|  Stacy|null|     F|  null|     100|  CS|\n",
      "|  199|   null|null|  null|     0|    null|  ML|\n",
      "|  289|  Zosso|null|     M|  null|      56|  ML|\n",
      "|   55|  Keith|null|     M|   270|      30|  CS|\n",
      "|   15|  Laura|  30|     F|   200|      37| Fin|\n",
      "+-----+-------+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.withColumn(\"Salary\", when((df_ps['Dept'] == 'ML') & (df_ps['Salary'] < 200), 0).otherwise(df_ps['Salary']*2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e739bf1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Using Groupby to group data by categories to provide summaries\n",
    "Below we try to draw some conclusion from grouping data by the Dept, Gender, and Age categories, using ```groupBy``` function. This should be intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8ca1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------+-----------+-------------+\n",
      "|Dept|        avg(empID)|avg(Age)|avg(Salary)|avg(Spending)|\n",
      "+----+------------------+--------+-----------+-------------+\n",
      "| Fin|             160.0|    31.2|      176.4|         55.0|\n",
      "|  CS|             119.0|   30.25|      150.2|         51.5|\n",
      "|  ML|137.28571428571428|    31.8|      322.5|         63.4|\n",
      "+----+------------------+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.groupBy('Dept').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bc656e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|Gender|sum(Spending)|\n",
      "+------+-------------+\n",
      "|     F|          436|\n",
      "|  null|          102|\n",
      "|     M|          418|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.groupBy('Gender').sum().select(['Gender', 'sum(Spending)']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bead051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Dept|count|\n",
      "+----+-----+\n",
      "| Fin|    6|\n",
      "|  CS|    6|\n",
      "|  ML|    7|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.groupBy('Dept').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c1d37",
   "metadata": {},
   "source": [
    "You can also groupby multiple columns. Below is one possible practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "986076ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------------+------------------+------------------+\n",
      "|Dept|Gender|          avg(Age)|       avg(Salary)|     avg(Spending)|\n",
      "+----+------+------------------+------------------+------------------+\n",
      "|  ML|     F|27.666666666666668|226.33333333333334| 66.33333333333333|\n",
      "| Fin|     M|              24.5|             115.5|50.333333333333336|\n",
      "| Fin|     F|              28.5|             125.5|              38.5|\n",
      "|  CS|     F|              30.0|             124.0|              80.0|\n",
      "|  CS|     M|30.333333333333332|            156.75|             37.25|\n",
      "|  ML|     M|              38.0|             575.5|              59.0|\n",
      "+----+------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.dropna(subset = ['Gender']).drop('empID').groupBy('Dept', 'Gender').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d636456",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "```groupBy``` is not needed to use aggregate functions, such as sum(), count(), etc. Below are examples of using aggregate functions without ```groupBy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a66a2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff932b55",
   "metadata": {},
   "source": [
    "total salary for all employees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3ecd698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|       3568|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.select(sum('Salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e51a15",
   "metadata": {},
   "source": [
    "average age of female employees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18a94d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    female_Avg_Age|\n",
      "+------------------+\n",
      "|28.333333333333332|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.filter(df_ps['Gender'] == 'F').select(avg('Age').alias('female_Avg_Age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89610c",
   "metadata": {},
   "source": [
    "total non-null entries in age columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ee946f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(Age)|\n",
      "+----------+\n",
      "|        14|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ps.select(count('Age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213ccca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Joining: Outer/Left/Right/Inner Join\n",
    "\n",
    "First, let's examine the 2 fake datasets below we've seen before again. The first is customer purchase history at some luxury brand, and second is employee data at company ABC. For ease of comparison, we remove all rows with any null values for both tables, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5cdd57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee = spark.read.csv('employees.csv', header = True, inferSchema = True).dropna()\n",
    "df_customer = spark.read.option(\"sep\",\"\\t\").csv('customers.txt', header = True, inferSchema = True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2d8645f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+-----+\n",
      "|CustomerName|ProductID| Product|Price|\n",
      "+------------+---------+--------+-----+\n",
      "|        John|     W311|  Wallet|  200|\n",
      "|      Jordan|     B203|    Belt|  500|\n",
      "|      Rachel|     B133|     Bag| 1000|\n",
      "|      Gordon|     B139|     Bag| 1200|\n",
      "|      Samuel|     B155|     Bag| 2300|\n",
      "|       Emily|     S152|   Shoes|  800|\n",
      "|       Alice|     W320|  Wallet|  250|\n",
      "|         Bob|     W320|  Wallet|  250|\n",
      "|      Daniel|     S300|Suitcase| 1200|\n",
      "|       Laura|     S305|Suitcase| 1790|\n",
      "|       Zosso|     W231|   Watch|  420|\n",
      "|       Evans|     DN18|    Deez| 2000|\n",
      "|      Rachel|     DN26|    Deez|10000|\n",
      "|         Bob|     DN26|    Deez|10000|\n",
      "+------------+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data extracted from customers.txt, with null value removed\n",
    "df_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7525b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---+------+------+--------+----+\n",
      "|empID|   Name|Age|Gender|Salary|Spending|Dept|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "|   13|Hatsune| 25|     F|   245|      50|  ML|\n",
      "|   23|  Alice| 30|     F|   270|      88|  ML|\n",
      "|  115|    Bob| 23|     M|   120|      46| Fin|\n",
      "|  123|Charlie| 25|     M|   133|      39|  CS|\n",
      "|  151| Gordon| 26|     M|   142|      35|  CS|\n",
      "|  152| Violet| 27|     F|   151|      40| Fin|\n",
      "|  156| Samuel| 26|     M|   111|      35| Fin|\n",
      "|  119| Rachel| 30|     F|   124|      60|  CS|\n",
      "|  157| Daniel| 31|     M|   151|      62|  ML|\n",
      "|  123|  Glenn| 40|     M|   217|      45|  CS|\n",
      "|  213| Shuang| 28|     F|   164|      61|  ML|\n",
      "|   15|  Laura| 30|     F|   100|      37| Fin|\n",
      "+-----+-------+---+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data extracted from employees.csv, with null value removed\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5097f5d",
   "metadata": {},
   "source": [
    "To perform any form of join, we specify the column(s) to join on and the join type. Below is an example for each type of join. We will only join on the name for this particular sets of tables, but for any join, the idea is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8dd519",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As can be seen, an outer join will join the two tables such that if an employee has no purchase records, it will just replace the entries on Customer table with ```null```. Similarly, if a customer has no employee records, its employee data will all be ```null```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d1bb6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+------+--------+----+------------+---------+--------+-----+\n",
      "|empID|   Name| Age|Gender|Salary|Spending|Dept|CustomerName|ProductID| Product|Price|\n",
      "+-----+-------+----+------+------+--------+----+------------+---------+--------+-----+\n",
      "| null|   null|null|  null|  null|    null|null|      Jordan|     B203|    Belt|  500|\n",
      "|  213| Shuang|  28|     F|   164|      61|  ML|        null|     null|    null| null|\n",
      "|  123|Charlie|  25|     M|   133|      39|  CS|        null|     null|    null| null|\n",
      "|   13|Hatsune|  25|     F|   245|      50|  ML|        null|     null|    null| null|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|         Bob|     W320|  Wallet|  250|\n",
      "|  115|    Bob|  23|     M|   120|      46| Fin|         Bob|     DN26|    Deez|10000|\n",
      "| null|   null|null|  null|  null|    null|null|        John|     W311|  Wallet|  200|\n",
      "|  156| Samuel|  26|     M|   111|      35| Fin|      Samuel|     B155|     Bag| 2300|\n",
      "| null|   null|null|  null|  null|    null|null|       Emily|     S152|   Shoes|  800|\n",
      "| null|   null|null|  null|  null|    null|null|       Evans|     DN18|    Deez| 2000|\n",
      "|   23|  Alice|  30|     F|   270|      88|  ML|       Alice|     W320|  Wallet|  250|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|      Rachel|     B133|     Bag| 1000|\n",
      "|  119| Rachel|  30|     F|   124|      60|  CS|      Rachel|     DN26|    Deez|10000|\n",
      "| null|   null|null|  null|  null|    null|null|       Zosso|     W231|   Watch|  420|\n",
      "|  123|  Glenn|  40|     M|   217|      45|  CS|        null|     null|    null| null|\n",
      "|  151| Gordon|  26|     M|   142|      35|  CS|      Gordon|     B139|     Bag| 1200|\n",
      "|  157| Daniel|  31|     M|   151|      62|  ML|      Daniel|     S300|Suitcase| 1200|\n",
      "|  152| Violet|  27|     F|   151|      40| Fin|        null|     null|    null| null|\n",
      "|   15|  Laura|  30|     F|   100|      37| Fin|       Laura|     S305|Suitcase| 1790|\n",
      "+-----+-------+----+------+------+--------+----+------------+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showCols = ['Name', 'Salary', 'CustomerName', 'Product']\n",
    "df_employee.join(df_customer, df_employee.Name == df_customer.CustomerName, 'outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac2822",
   "metadata": {},
   "source": [
    "Next is inner join. As seen below, only the people who are both employees and customer (from the tables shown above) show up. Those who are only employees or only customers are removed from the joined table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "357af91d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+------+------+--------+----+------------+---------+--------+-----+\n",
      "|empID|  Name|Age|Gender|Salary|Spending|Dept|CustomerName|ProductID| Product|Price|\n",
      "+-----+------+---+------+------+--------+----+------------+---------+--------+-----+\n",
      "|   23| Alice| 30|     F|   270|      88|  ML|       Alice|     W320|  Wallet|  250|\n",
      "|  115|   Bob| 23|     M|   120|      46| Fin|         Bob|     DN26|    Deez|10000|\n",
      "|  115|   Bob| 23|     M|   120|      46| Fin|         Bob|     W320|  Wallet|  250|\n",
      "|  151|Gordon| 26|     M|   142|      35|  CS|      Gordon|     B139|     Bag| 1200|\n",
      "|  156|Samuel| 26|     M|   111|      35| Fin|      Samuel|     B155|     Bag| 2300|\n",
      "|  119|Rachel| 30|     F|   124|      60|  CS|      Rachel|     DN26|    Deez|10000|\n",
      "|  119|Rachel| 30|     F|   124|      60|  CS|      Rachel|     B133|     Bag| 1000|\n",
      "|  157|Daniel| 31|     M|   151|      62|  ML|      Daniel|     S300|Suitcase| 1200|\n",
      "|   15| Laura| 30|     F|   100|      37| Fin|       Laura|     S305|Suitcase| 1790|\n",
      "+-----+------+---+------+------+--------+----+------------+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.join(df_customer, df_employee.Name == df_customer.CustomerName, 'inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c19d8",
   "metadata": {},
   "source": [
    "A left join as seen below keeps the employees in the table even if they are not customers. Their customer details will just be padded with null. However, it will remove the customers who are not employees from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c8aae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---+------+------+--------+----+------------+---------+--------+-----+\n",
      "|empID|   Name|Age|Gender|Salary|Spending|Dept|CustomerName|ProductID| Product|Price|\n",
      "+-----+-------+---+------+------+--------+----+------------+---------+--------+-----+\n",
      "|   13|Hatsune| 25|     F|   245|      50|  ML|        null|     null|    null| null|\n",
      "|   23|  Alice| 30|     F|   270|      88|  ML|       Alice|     W320|  Wallet|  250|\n",
      "|  115|    Bob| 23|     M|   120|      46| Fin|         Bob|     DN26|    Deez|10000|\n",
      "|  115|    Bob| 23|     M|   120|      46| Fin|         Bob|     W320|  Wallet|  250|\n",
      "|  123|Charlie| 25|     M|   133|      39|  CS|        null|     null|    null| null|\n",
      "|  151| Gordon| 26|     M|   142|      35|  CS|      Gordon|     B139|     Bag| 1200|\n",
      "|  152| Violet| 27|     F|   151|      40| Fin|        null|     null|    null| null|\n",
      "|  156| Samuel| 26|     M|   111|      35| Fin|      Samuel|     B155|     Bag| 2300|\n",
      "|  119| Rachel| 30|     F|   124|      60|  CS|      Rachel|     DN26|    Deez|10000|\n",
      "|  119| Rachel| 30|     F|   124|      60|  CS|      Rachel|     B133|     Bag| 1000|\n",
      "|  157| Daniel| 31|     M|   151|      62|  ML|      Daniel|     S300|Suitcase| 1200|\n",
      "|  123|  Glenn| 40|     M|   217|      45|  CS|        null|     null|    null| null|\n",
      "|  213| Shuang| 28|     F|   164|      61|  ML|        null|     null|    null| null|\n",
      "|   15|  Laura| 30|     F|   100|      37| Fin|       Laura|     S305|Suitcase| 1790|\n",
      "+-----+-------+---+------+------+--------+----+------------+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.join(df_customer, df_employee.Name == df_customer.CustomerName, 'left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca4643d",
   "metadata": {},
   "source": [
    "A right join just works the opposite way as the left join, i.e. it keeps all customers but removes employee data if employee is not also a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1975b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+------+------+--------+----+------------+---------+--------+-----+\n",
      "|empID|  Name| Age|Gender|Salary|Spending|Dept|CustomerName|ProductID| Product|Price|\n",
      "+-----+------+----+------+------+--------+----+------------+---------+--------+-----+\n",
      "| null|  null|null|  null|  null|    null|null|        John|     W311|  Wallet|  200|\n",
      "| null|  null|null|  null|  null|    null|null|      Jordan|     B203|    Belt|  500|\n",
      "|  119|Rachel|  30|     F|   124|      60|  CS|      Rachel|     B133|     Bag| 1000|\n",
      "|  151|Gordon|  26|     M|   142|      35|  CS|      Gordon|     B139|     Bag| 1200|\n",
      "|  156|Samuel|  26|     M|   111|      35| Fin|      Samuel|     B155|     Bag| 2300|\n",
      "| null|  null|null|  null|  null|    null|null|       Emily|     S152|   Shoes|  800|\n",
      "|   23| Alice|  30|     F|   270|      88|  ML|       Alice|     W320|  Wallet|  250|\n",
      "|  115|   Bob|  23|     M|   120|      46| Fin|         Bob|     W320|  Wallet|  250|\n",
      "|  157|Daniel|  31|     M|   151|      62|  ML|      Daniel|     S300|Suitcase| 1200|\n",
      "|   15| Laura|  30|     F|   100|      37| Fin|       Laura|     S305|Suitcase| 1790|\n",
      "| null|  null|null|  null|  null|    null|null|       Zosso|     W231|   Watch|  420|\n",
      "| null|  null|null|  null|  null|    null|null|       Evans|     DN18|    Deez| 2000|\n",
      "|  119|Rachel|  30|     F|   124|      60|  CS|      Rachel|     DN26|    Deez|10000|\n",
      "|  115|   Bob|  23|     M|   120|      46| Fin|         Bob|     DN26|    Deez|10000|\n",
      "+-----+------+----+------+------+--------+----+------------+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.join(df_customer, df_employee.Name == df_customer.CustomerName, 'right').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3a79e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Other possibly useful features\n",
    "\n",
    "First, let's take a dataset like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abadcb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|_c0|_c1|\n",
      "+---+---+\n",
      "|  1|  0|\n",
      "|  1|  1|\n",
      "|  1|  2|\n",
      "|  2|  1|\n",
      "|  0|  1|\n",
      "|  1|  2|\n",
      "|  2|  2|\n",
      "|  1|  1|\n",
      "|  1|  0|\n",
      "|  1|  0|\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  0|\n",
      "|  0|  2|\n",
      "|  0|  2|\n",
      "|  0|  1|\n",
      "|  0|  0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_labels = spark.read.csv('labels.csv', inferSchema = 'True')\n",
    "df_labels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22680c",
   "metadata": {},
   "source": [
    "We can count the number of matching occurences in the two columns via a count combined with conditional, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94411c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e56a8c0",
   "metadata": {},
   "source": [
    "Note: Of course, if you join the tables in the other way, i.e. join employees to customer table, then left/right join will produce the opposite effect to above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edf01b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import to html\n",
    "import os\n",
    "\n",
    "os.system('jupyter nbconvert --to html Basic_Data_Manip.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
